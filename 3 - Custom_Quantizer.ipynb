{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2URh8ZEt4qe"
      },
      "source": [
        "# Quantizing Weights & Activations for Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0mSZC4kct4qg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from helper import linear_quantization_symm, get_q_scale_symmetric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lS5rnyc6t4qh"
      },
      "outputs": [],
      "source": [
        "def quantized_linear_W8A32_without_bias(input,q_w,s_w,z_w):\n",
        "    assert input.dtype == torch.float32\n",
        "    assert q_w.dtype == torch.int8\n",
        "    dequantized_w = s_w*(q_w.to(torch.int32)-z_w)\n",
        "    output = torch.nn.functional.linear(input,dequantized_w)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cLBR4mjYt4qh"
      },
      "outputs": [],
      "source": [
        "input = torch.tensor([1, 2, 3], dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wtjnyDWFt4qi"
      },
      "outputs": [],
      "source": [
        "weight = torch.tensor([[-2,   -1.13, 0.42],\n",
        "                       [-1.51, 0.25, 1.62],\n",
        "                       [0.23,  1.35, 2.15]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WPbJxpLlt4qi"
      },
      "outputs": [],
      "source": [
        "q_w, s_w  = linear_quantization_symm(weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1K9TgFpt4qi",
        "outputId": "9f768a15-bcf6-4386-a5f8-2b2bedc95dd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantized Weights = tensor([[-118,  -67,   25],\n",
            "        [ -89,   15,   96],\n",
            "        [  14,   80,  127]], dtype=torch.int8)\n",
            "Scale = 0.016929134609192376\n"
          ]
        }
      ],
      "source": [
        "print (\"Quantized Weights =\",q_w)\n",
        "print (\"Scale =\",s_w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bMMhg_4It4qj"
      },
      "outputs": [],
      "source": [
        "output_w_q = quantized_linear_W8A32_without_bias(input,q_w,s_w,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-bPcwAuut4qj"
      },
      "outputs": [],
      "source": [
        "output_wo_q = torch.nn.functional.linear(input,weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnQ25rsot4qj",
        "outputId": "8c7389d7-3f6a-4154-99e4-aa3f0dc2a196"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W8A32 Output:\n",
            "tensor([-2.9965,  3.8768,  9.3957])\n"
          ]
        }
      ],
      "source": [
        "print (\"W8A32 Output:\")\n",
        "print (output_w_q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWSY2L8Mt4qk",
        "outputId": "2c1f6aeb-a114-469d-9b69-b073ff84b3aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output Without Quantization:\n",
            "tensor([-3.0000,  3.8500,  9.3800])\n"
          ]
        }
      ],
      "source": [
        "print (\"Output Without Quantization:\")\n",
        "print (output_wo_q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ckaqwt_Jt4ql"
      },
      "source": [
        "# Custom Build an 8-Bit Quantizer\n",
        "We create a class `W8A16LinearLayer`, which will be responsible for quantizing the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VPuJsHD0t4ql"
      },
      "outputs": [],
      "source": [
        "random_int8 = torch.randint(-128,127, (32,16)).to(dtype = torch.int8)\n",
        "random_hs = torch.randn((1, 16), dtype=torch.bfloat16)\n",
        "scales = torch.randn((1, 32), dtype=torch.bfloat16)\n",
        "bias = torch.randn((1, 32), dtype=torch.bfloat16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nIXKg_TOt4qm"
      },
      "outputs": [],
      "source": [
        "def w8_a16_forward(input, weight, scale, bias = None):\n",
        "    weight = weight.to(dtype = input.dtype)\n",
        "    if (bias == None):\n",
        "        return torch.nn.functional.linear(input,weight)*scale\n",
        "    return torch.nn.functional.linear(input,weight)*scale + bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ye3Wgjxt4qm",
        "outputId": "8e004927-c1af-4f34-fa07-748c826ee71b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With bias:\n",
            "\n",
            " tensor([[ -412.0000,   308.0000,    95.5000,    51.7500,   680.0000,  -124.5000,\n",
            "          1168.0000,   -43.0000,   165.0000,   324.0000,  -474.0000,  -122.5000,\n",
            "          -288.0000,   944.0000,   608.0000,   258.0000,   180.0000,  -121.0000,\n",
            "         -1344.0000,   -33.7500,    20.1250,  -139.0000,   280.0000,  -203.0000,\n",
            "          -219.0000,  -183.0000,   580.0000,  -123.0000,  -202.0000,    58.0000,\n",
            "           160.0000,    -6.5312]], dtype=torch.bfloat16)\n",
            "\n",
            "Without bias:\n",
            "\n",
            " tensor([[ -414.0000,   308.0000,    95.5000,    51.7500,   680.0000,  -124.5000,\n",
            "          1168.0000,   -42.0000,   164.0000,   324.0000,  -474.0000,  -121.0000,\n",
            "          -288.0000,   944.0000,   608.0000,   258.0000,   180.0000,  -121.5000,\n",
            "         -1344.0000,   -34.0000,    21.3750,  -141.0000,   280.0000,  -203.0000,\n",
            "          -220.0000,  -183.0000,   580.0000,  -124.0000,  -200.0000,    59.0000,\n",
            "           161.0000,    -5.6875]], dtype=torch.bfloat16)\n"
          ]
        }
      ],
      "source": [
        "print(\"With bias:\\n\\n\", w8_a16_forward(random_hs, random_int8, scales, bias))\n",
        "\n",
        "print(\"\\nWithout bias:\\n\\n\", w8_a16_forward(random_hs, random_int8, scales))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "umkzkwEwt4qm"
      },
      "outputs": [],
      "source": [
        "class W8A16LinearLayer(nn.Module):\n",
        "    def __init__(self,input_features,output_features, bias = True, dtype = torch.float32):\n",
        "        super().__init__()\n",
        "\n",
        "        self.register_buffer('int8_weights', torch.randint(-128,127, (output_features,input_features)).to(torch.int8))\n",
        "        self.register_buffer('scales', torch.randn((output_features),dtype= dtype))\n",
        "        if bias:\n",
        "            self.register_buffer('bias', torch.randn((1,output_features), dtype = dtype))\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "\n",
        "    def quantize(self,weights):\n",
        "        r_max = weights.clone().abs().max(dim=-1).values\n",
        "        q_max = torch.iinfo(torch.int8).max\n",
        "        scales = r_max/q_max\n",
        "        scales = scales.to(weights.dtype)\n",
        "        self.scales = r_max/q_max\n",
        "        self.int8_weights = torch.round(weights/self.scales.unsqueeze(1)).to(torch.int8)\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        return w8_a16_forward(input, self.int8_weights, self.scales, self.bias)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HFgR_OLit4qn"
      },
      "outputs": [],
      "source": [
        "module = W8A16LinearLayer(4, 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vERjHcEFt4qn",
        "outputId": "2c591613-c24b-4ce0-f644-b5cb4a70026b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  71,  -78,   -4,   10],\n",
            "        [ 101,  -21,  -42,   30],\n",
            "        [-100,   61,  -76,  -44],\n",
            "        [  82,   20,   90,  -45],\n",
            "        [ -86,   47,   97,  -35],\n",
            "        [  14,   35,  -56,    9],\n",
            "        [ -47,  -31,  -51,  -43],\n",
            "        [ -59,  -94,  -93, -107]], dtype=torch.int8)\n"
          ]
        }
      ],
      "source": [
        "print (module.int8_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5h4hmH2jt4qn"
      },
      "outputs": [],
      "source": [
        "# random_matrix = torch.randn((4, 8), dtype=torch.bfloat16)\n",
        "random_matrix = torch.tensor([[ 0.4668,  0.3750, -0.2969,  0.9180,  1.2578,  0.1089,  0.0000,  1.0703],\n",
        "        [-0.9805,  0.0649, -0.6484, -0.1465, -1.4219,  0.0615,  0.0000,  0.0000],\n",
        "        [-0.5039, -1.9219,  0.0713,  1.0625,  0.0347, -1.3203, -0.1699,  1.4922],\n",
        "        [ 0.0403, -0.7891, -0.5391,  0.9141,  1.4531, -1.0859, -0.3398, -0.4336]],\n",
        "       dtype=torch.bfloat16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5dJX5set4qo",
        "outputId": "6118b726-f747-44de-c950-31098db7eaf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  47,   38,  -30,   93,  127,   11,    0,  108],\n",
            "        [ -88,    6,  -58,  -13, -128,    6,    0,    0],\n",
            "        [ -33, -127,    5,   70,    2,  -87,  -11,   98],\n",
            "        [   4,  -69,  -47,   80, -128,  -95,  -30,  -38]], dtype=torch.int8)\n"
          ]
        }
      ],
      "source": [
        "module.quantize(random_matrix)\n",
        "print (module.int8_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Svpvx5GUt4qo",
        "outputId": "ffde0f7a-bfc0-40fa-c251-99c2b92a2c9e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0099, 0.0112, 0.0151, 0.0114], dtype=torch.bfloat16)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "module.scales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1qXLatjt4qo",
        "outputId": "5b1b79da-9817-4218-cf8b-b37d7931ea69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8])\n",
            "torch.Size([4])\n"
          ]
        }
      ],
      "source": [
        "print (module.int8_weights.shape)\n",
        "print (module.scales.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fOnBHXNt4qp",
        "outputId": "c5915218-f840-479e-8876-a9d18dc31dc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.4648,  0.3750, -0.2969,  0.9180,  1.2578,  0.1089,  0.0000,  1.0703],\n",
            "        [-0.9844,  0.0669, -0.6484, -0.1455, -1.4297,  0.0669,  0.0000,  0.0000],\n",
            "        [-0.5000, -1.9219,  0.0757,  1.0625,  0.0303, -1.3203, -0.1660,  1.4844],\n",
            "        [ 0.0457, -0.7891, -0.5352,  0.9141, -1.4609, -1.0859, -0.3418, -0.4336]],\n",
            "       dtype=torch.bfloat16)\n"
          ]
        }
      ],
      "source": [
        "dequantized_wt = module.scales.unsqueeze(1) *  module.int8_weights\n",
        "print (dequantized_wt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klHkI32qt4qp",
        "outputId": "f70f3d77-2299-4645-919b-7abd129e2a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.4668,  0.3750, -0.2969,  0.9180,  1.2578,  0.1089,  0.0000,  1.0703],\n",
            "        [-0.9805,  0.0649, -0.6484, -0.1465, -1.4219,  0.0615,  0.0000,  0.0000],\n",
            "        [-0.5039, -1.9219,  0.0713,  1.0625,  0.0347, -1.3203, -0.1699,  1.4922],\n",
            "        [ 0.0403, -0.7891, -0.5391,  0.9141,  1.4531, -1.0859, -0.3398, -0.4336]],\n",
            "       dtype=torch.bfloat16)\n"
          ]
        }
      ],
      "source": [
        "print (random_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mQ4T13tt4qq"
      },
      "source": [
        "# Quantization Pipeline\n",
        "- Replace all of the `torch.nn.Linear` layers with the `W8A16LinearLayer` layer.\n",
        "- Call `quantize` on the linear layers using the original weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piujUlMst4qq"
      },
      "source": [
        "## 1. Linear Layer Replacement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "GwBMuFrAt4qq"
      },
      "outputs": [],
      "source": [
        "def replace_linear_with_target(module,target_class, excluded_module):\n",
        "    for name,child in module.named_children():\n",
        "        if (isinstance(child, nn.Linear) and not any([x==name for x in excluded_module])):\n",
        "            old_bias = child.bias\n",
        "            new_module = target_class(child.in_features,\n",
        "                                      child.out_features,\n",
        "                                      old_bias is not None,\n",
        "                                      child.weight.dtype)\n",
        "            setattr(module, name, new_module)\n",
        "            if old_bias != None:\n",
        "                getattr(module,name).bias = old_bias\n",
        "        else:\n",
        "            replace_linear_with_target(child,target_class,excluded_module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "BVNmGgwst4qr"
      },
      "outputs": [],
      "source": [
        "class Test_Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.emb = torch.nn.Embedding(3,1)\n",
        "        self.l1 = nn.Linear(2,3)\n",
        "        self.l2 = nn.Linear(14,4, bias= False)\n",
        "        self.lm_head = nn.Linear(2, 1, bias=False)\n",
        "\n",
        "t1 = Test_Model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ez0P9dt8t4qr",
        "outputId": "ac855917-b6b2-4f6e-c38d-39a54794f3b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test_Model(\n",
            "  (emb): Embedding(3, 1)\n",
            "  (l1): W8A16LinearLayer()\n",
            "  (l2): W8A16LinearLayer()\n",
            "  (lm_head): W8A16LinearLayer()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "replace_linear_with_target(t1,W8A16LinearLayer,[])\n",
        "print (t1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqE4jiR2t4qr"
      },
      "source": [
        "## 2. Layer Replacement + Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "1vnhxvB4t4qs"
      },
      "outputs": [],
      "source": [
        "def replace_linear_with_target_and_quantize(module,target_class, excluded_module):\n",
        "    for name,child in module.named_children():\n",
        "        if (isinstance(child, nn.Linear) and not any([x==name for x in excluded_module])):\n",
        "            old_bias = child.bias\n",
        "            old_wt = child.weight\n",
        "            new_module = target_class(child.in_features,\n",
        "                                      child.out_features,\n",
        "                                      old_bias is not None,\n",
        "                                      child.weight.dtype)\n",
        "            new_module.quantize(old_wt)\n",
        "            setattr(module, name, new_module)\n",
        "\n",
        "            # print(\"Before Wt: \", old_wt)\n",
        "            # print(\"After Wt: \", getattr(module,name).int8_weights, '\\n')\n",
        "\n",
        "            if old_bias != None:\n",
        "                getattr(module,name).bias = old_bias\n",
        "\n",
        "        else:\n",
        "            replace_linear_with_target(child,target_class,excluded_module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Lv7S2iYmt4qs"
      },
      "outputs": [],
      "source": [
        "class Test_Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.emb = torch.nn.Embedding(4,4)\n",
        "        self.l1 = nn.Linear(4,4)\n",
        "        self.l2 = nn.Linear(4,4, bias= False)\n",
        "        self.lm_head = nn.Linear(4,4, bias=False)\n",
        "\n",
        "t1 = Test_Model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYCxmHo7t4qt",
        "outputId": "6feb66df-133f-4e44-db02-1885f03f2790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test_Model(\n",
            "  (emb): Embedding(4, 4)\n",
            "  (l1): Linear(in_features=4, out_features=4, bias=True)\n",
            "  (l2): Linear(in_features=4, out_features=4, bias=False)\n",
            "  (lm_head): Linear(in_features=4, out_features=4, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print (t1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIi3GFHDt4qu",
        "outputId": "8c5710f9-eefb-4ef4-829c-a46e8db9cad3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test_Model(\n",
            "  (emb): Embedding(4, 4)\n",
            "  (l1): W8A16LinearLayer()\n",
            "  (l2): W8A16LinearLayer()\n",
            "  (lm_head): Linear(in_features=4, out_features=4, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "replace_linear_with_target_and_quantize(t1,W8A16LinearLayer, ['lm_head'])\n",
        "print (t1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkeR8w8Rt4qu"
      },
      "source": [
        "We can observe that our function is able to replace the required nn.Linear layers in the model t1"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ML",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}